---
title: "Symbiodiniaceae ITS2 intragenomic variants from cultures: Bioinformatic pipeline and sequence similarity networks"
author: "MR Nitschke"
date: "16/12/2019"
---
Following the dada2 workflow here https://benjjneb.github.io/dada2/tutorial.html, and here https://benjjneb.github.io/dada2/ITS_workflow.html

# Libraries

```{r}
library(dada2)
library(tidyverse)
theme_set(theme_bw())
library(ShortRead)
library(Biostrings)
library(phyloseq)

```

# Staging. Cutadapt for demultiplexing and barcode trimming

```{r}
cutadapt -g file:barcodes.fasta -o demuxtrim-{name}.R1.fastq.gz -p demuxtrim-{name}.R2.fastq.gz SAM1-22_S3_L001_R1_001.fastq.gz SAM1-22_S3_L001_R2_001.fastq.gz

## Because both R1 and R2 files contain forward and reverse reads (MrDNAs particular library prep protocol), but only the barcodes on the forward read determine where cutadapt writes each sequence, a second round of cutadapt must be run on those reads that failed to demultiplex on round 1 (explained in more detail here https://github.com/marcelm/cutadapt/issues/356#issuecomment-463932533). This is because Cutadapt only uses the first input to assign where a read is written, and the rest are written  to 'unknowns'.

cutadapt -g file:barcodes.fasta -o demuxtrim-{name}.R2.swap.fastq.gz -p demuxtrim-{name}.R1.swap.fastq.gz demuxtrim-un
known.R2.fastq.gz demuxtrim-unknown.R1.fastq.gz
```

# 1. DADA2 on Forward (1st round)

## Set up the directory of each batch. 
Turn on and off using # as each batch is completed

```{r}
# Batch 1:
path <- "../Raw_data/Culture_IGV/"

list.files(path) # Check before moving onwards that you have the right path established
```

## Check for primers

```{r}
# Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq
fnFs <- sort(list.files(path, pattern = "R1.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "R2.fastq", full.names = TRUE))
```

```{r}
# Define Symbiodinium ITS2 primers
FWD <- "GTGAATTGCAGAACTCCGTG"
REV <- "CCTCCGCTTACTTATATGCTT"
```

```{r}
allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna),
        RevComp = reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
REV.orients
```

```{r}
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs[[1]]),
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs[[1]]),
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs[[1]]),
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs[[1]]))

# After running the primer search function above on the demultiplexed files we see that all the primers are still on the forwards and reverse reads. Use cutadapt to remove these.
```

## Cutadapt bash script

Bash script to remove primers in all orientations:

```{}
bash cutadapt_trim_primers_b.bash
```

This will launch the looped code below:

```{r}
for i in *.R1.fastq.gz
do
  SAMPLE=$(echo ${i} | sed "s/.R1\.fastq\.gz//") 
  echo ${SAMPLE}.R1.fastq.gz ${SAMPLE}.R2.fastq.gz
cutadapt -b file:primers.fasta -B file:primers.fasta -n 2 -m 50 -o ${SAMPLE}.R1.CUT.fastq.gz -p ${SAMPLE}.R2.CUT.fastq.gz ${SAMPLE}.R1.fastq.gz ${SAMPLE}.R2.fastq.gz
done
```

```{r}
# List new directory of 'Cuts'. These are the files generated by the bash script above that have .CUT. in the filename.
path <- "../Raw_data/Culture_IGV/Cuts/"
list.files(path)
```

```{r}
# Pattern search again for R1.CUT and R2.CUT in filenames
cutFs <- sort(list.files(path, pattern = "R1.CUT.fastq", full.names = TRUE))
cutRs <- sort(list.files(path, pattern = "R2.CUT.fastq", full.names = TRUE))
```

```{r}
# Check how well cutadapt worked at removing primers

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = cutFs[[1]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = cutRs[[1]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = cutFs[[1]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = cutRs[[1]]))

## All primers removed!
```

```{r}
# Two rounds of strsplit to remove head and tail of the file name and keep the sample name.
sample.names <- sapply(strsplit(basename(cutFs), ".R1"), `[`, 1)
sample.names <- sapply(strsplit(sample.names, "demuxtrim-"), `tail`, 1)
```

```{r}
# Check quality of sequences of Forward reads
plotQualityProfile(cutFs[1:3])
```

```{r}
# Check quality of sequences of Reverse reads
plotQualityProfile(cutRs[1:3])
```

```{r}
# Set up subdirectory for placing the filtered sequences
filtFs <- file.path(path, "filtered", paste0(sample.names, "_R1_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R2_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```

```{r}
# Filter the cutFs and cutRs into the directories created above. maxN = 0 because dada2 cannot have sequences with ambiguous bases. maxEE = c(2, 4): relaxed the EE to 4 for the reverse reads which typically are of lower quality.

out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(2, 4), truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = FALSE) # On Windows set multithread=FALSE

# Check how many reads make it through filtering
head(out) 
```

```{r}
# Create an error model for the forward reads
errF <- learnErrors(filtFs, multithread = FALSE)
```

```{r}
# Create an error model for the forward reads
errR <- learnErrors(filtRs, multithread = FALSE)
```

```{r}
# Check that the error model fits the errors
plotErrors(errF, nominalQ = TRUE)
```

## Denoise and infer samples

```{r}
# Run the core dada2 method to remove noise and infer samples on forward reads
dadaFs <- dada(filtFs, err = errF, multithread = FALSE)
```

```{r}
# Run the core dada2 method to remove noise and infer samples on reverse reads
dadaRs <- dada(filtRs, err = errR, multithread = FALSE)
```

```{r}
# Merge the forward and reverse reads
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose = TRUE)
```

```{r}
# Turn the successfully merged sequences into a sequence table (i.e. an 'otu table' or 'asv table')
seqtab <- makeSequenceTable(mergers)
```

## Save point

```{r}
saveRDS(seqtab, "round1.rds")
```

# 2. DADA2 on swapped (second round)

## Set up the directory of each batch. 

Because both R1_001.fastq.gz and R2_001.fastq.gz contain forward and reverse reads (MrDNAs particular library prep protocol), but barcodes only on the forward read determine where cutadapt writes each sequence, a second round of dada2 must be run on those reads that failed to demultiplex on round 1 (explained in more detail here https://github.com/marcelm/cutadapt/issues/356#issuecomment-463932533). We must repeat everything we have done above, with R2.swapped in the place of R1.swapped, and vice versa. This is because the R2 reads of lower quality are in the forwards orientation now, so we must re-learn the error models and combine the rounds later.

Turn on and off using # as each batch is completed

```{r}
path <- "Raw/SAM1-24-258095994/" # Add notes here to remind you which batch has been completed

list.files(path) # Check before moving onwards that you have the right path established
```

## Check for primers

```{r}
# Forward and reverse fastq filenames have format: SAMPLENAME.R2.swap.fastq and SAMPLENAME.R1.swap.fastq
# NOTE: The swapping of the position of R2 and R1.
fnFs <- sort(list.files(path, pattern = "R2.swap.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "R1.swap.fastq", full.names = TRUE))
```

```{r}
# Define Symbiodinium ITS2 primers
FWD <- "GTGAATTGCAGAACTCCGTG"
REV <- "CCTCCGCTTACTTATATGCTT"
```

```{r}
allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna),
        RevComp = reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
REV.orients
```

```{r}
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs[[1]]),
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs[[1]]),
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs[[1]]),
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs[[1]]))
```

## Cutadapt bash script

Bash script to remove primers in all orientations:

```{}
bash cutadapt_trim_primers_b_swap.bash
```

This will launch the looped code below: NOTE THE SWAPPED POSITIONS OF R1 AND R2!!!!!!!!

```{r}
for i in *.R2.swap.fastq.gz
do
  SAMPLE=$(echo ${i} | sed "s/.R2\.swap\.fastq\.gz//") 
  echo ${SAMPLE}.R2.swap.fastq.gz ${SAMPLE}.R1.swap.fastq.gz
cutadapt -b file:primers.fasta -B file:primers.fasta -n 2 -m 50 -o ${SAMPLE}.R2.swap.CUT.fastq.gz -p ${SAMPLE}.R1.swap.CUT.fastq.gz ${SAMPLE}.R2.swap.fastq.gz ${SAMPLE}.R1.swap.fastq.gz
done
```

```{r}
# List new directory of 'cuts', i.e. the primer removed fwd and revs
path <- "../Raw_data/Culture_IGV/Cuts_swap/"
list.files(path)
```

```{r}
# Pattern search again for forward and reverse in filenames
# NOTE: The swapping of the position of R2 and R1.
cutFs <- sort(list.files(path, pattern = "R2.swap.CUT.fastq", full.names = TRUE))
cutRs <- sort(list.files(path, pattern = "R1.swap.CUT.fastq", full.names = TRUE))
```

```{r}
# Check how well cutadapt worked at removing primers

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = cutFs[[1]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = cutRs[[1]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = cutFs[[1]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = cutRs[[1]]))

# All primers removed!
```

```{r}
sample.names <- sapply(strsplit(basename(cutFs), ".R2"), `[`, 1)
sample.names <- sapply(strsplit(sample.names, "demuxtrim-"), `tail`, 1)
sample.names <- paste(sample.names, ".swap", sep = "")
```

```{r}
plotQualityProfile(cutFs[1:3])

# Note: now the cutFs have the reverse sequencing quality that is typically lower than the forward, due to the switching of positions of R1 and R2
```

```{r}
plotQualityProfile(cutRs[1:3])

# Note: now the cutRs have the forward sequencing quality that is typically higher than the reverse, due to the switching of positions of R1 and R2
```

```{r}
# Place filtered files in filtered/ subdirectory
# NOTE: The swapping of the position of R2 and R1.
filtFs <- file.path(path, "filtered", paste0(sample.names, "_R2_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R1_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```

```{r}
# Swapped the maxEE around as now reverse reads are in the forwards position and vice versa, as described above.
out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(4, 2), truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = FALSE) # On Windows set multithread=FALSE

# Check how many reads make it through filtering
head(out) 
```

```{r}
# Create an error model for the forward reads
errF <- learnErrors(filtFs, multithread = FALSE)
```

```{r}
# Create an error model for the forward reads
errR <- learnErrors(filtRs, multithread = FALSE)
```

```{r}
# Check that the error model fits the errors
plotErrors(errF, nominalQ = TRUE)
```

## Denoise and infer samples

```{r}
# Run the core dada2 method to remove noise and infer samples on forward reads
dadaFs <- dada(filtFs, err = errF, multithread = FALSE)
```

```{r}
# Run the core dada2 method to remove noise and infer samples on reverse reads
dadaRs <- dada(filtRs, err = errR, multithread = FALSE)
```

```{r}
# Merge the forward and reverse reads
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose = TRUE)
```

```{r}
# Turn the successfully merged sequences into a sequence table (i.e. an 'otu table' or 'asv table')
seqtab <- makeSequenceTable(mergers)
```

## Save point

```{r}
saveRDS(seqtab, "round2.rds")
```

# 3. Merge rounds

```{r}
library(dplyr)
library(tibble)
library(stringr)

# Function that sums the Forward and Swap samples into one row
sum_swapped <- function(data, ...) {
  data = as.data.frame(data)
  data = tibble::rownames_to_column(data, var = "SampleID")
  data$SampleID = stringr::str_remove(data$SampleID, ".swap_R2_filt.fastq.gz")
  data = data %>% 
     group_by(SampleID) %>% 
     summarise_all(list(sum))
  rownames(data) <- data$SampleID
  data = data %>%
    select(-SampleID)
  data = as.matrix(data)
}

### Join first and second round

Forward <- readRDS("round1.rds")
Swap <- readRDS("round2.rds")
All <- mergeSequenceTables(Forward, Swap)

complete_seqtab <- sum_swapped(All)
saveRDS(complete_seqtab, "complete_seqtab.rds")
```

